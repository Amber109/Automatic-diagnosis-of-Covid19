{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# data loader\n",
    "from __future__ import print_function, division\n",
    "import glob\n",
    "import torch\n",
    "from skimage import io, transform, color\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "\n",
    "#==========================dataset load==========================\n",
    "class RescaleT(object):\n",
    "\n",
    "\tdef __init__(self,output_size):\n",
    "\t\tassert isinstance(output_size,(int,tuple))\n",
    "\t\tself.output_size = output_size\n",
    "\n",
    "\tdef __call__(self,sample):\n",
    "\t\timidx, image, label = sample['imidx'], sample['image'],sample['label']\n",
    "\n",
    "\t\th, w = image.shape[:2]\n",
    "\n",
    "\t\tif isinstance(self.output_size,int):\n",
    "\t\t\tif h > w:\n",
    "\t\t\t\tnew_h, new_w = self.output_size*h/w,self.output_size\n",
    "\t\t\telse:\n",
    "\t\t\t\tnew_h, new_w = self.output_size,self.output_size*w/h\n",
    "\t\telse:\n",
    "\t\t\tnew_h, new_w = self.output_size\n",
    "\n",
    "\t\tnew_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "\t\t# #resize the image to new_h x new_w and convert image from range [0,255] to [0,1]\n",
    "\t\t# img = transform.resize(image,(new_h,new_w),mode='constant')\n",
    "\t\t# lbl = transform.resize(label,(new_h,new_w),mode='constant', order=0, preserve_range=True)\n",
    "\n",
    "\t\timg = transform.resize(image,(self.output_size,self.output_size),mode='constant')\n",
    "\t\tlbl = transform.resize(label,(self.output_size,self.output_size),mode='constant', order=0, preserve_range=True)\n",
    "\n",
    "\t\treturn {'imidx':imidx, 'image':img,'label':lbl}\n",
    "\n",
    "class Rescale(object):\n",
    "\n",
    "\tdef __init__(self,output_size):\n",
    "\t\tassert isinstance(output_size,(int,tuple))\n",
    "\t\tself.output_size = output_size\n",
    "\n",
    "\tdef __call__(self,sample):\n",
    "\t\timidx, image, label = sample['imidx'], sample['image'],sample['label']\n",
    "\n",
    "\t\tif random.random() >= 0.5:\n",
    "\t\t\timage = image[::-1]\n",
    "\t\t\tlabel = label[::-1]\n",
    "\n",
    "\t\th, w = image.shape[:2]\n",
    "\n",
    "\t\tif isinstance(self.output_size,int):\n",
    "\t\t\tif h > w:\n",
    "\t\t\t\tnew_h, new_w = self.output_size*h/w,self.output_size\n",
    "\t\t\telse:\n",
    "\t\t\t\tnew_h, new_w = self.output_size,self.output_size*w/h\n",
    "\t\telse:\n",
    "\t\t\tnew_h, new_w = self.output_size\n",
    "\n",
    "\t\tnew_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "\t\t# #resize the image to new_h x new_w and convert image from range [0,255] to [0,1]\n",
    "\t\timg = transform.resize(image,(new_h,new_w),mode='constant')\n",
    "\t\tlbl = transform.resize(label,(new_h,new_w),mode='constant', order=0, preserve_range=True)\n",
    "\n",
    "\t\treturn {'imidx':imidx, 'image':img,'label':lbl}\n",
    "\n",
    "class RandomCrop(object):\n",
    "\n",
    "\tdef __init__(self,output_size):\n",
    "\t\tassert isinstance(output_size, (int, tuple))\n",
    "\t\tif isinstance(output_size, int):\n",
    "\t\t\tself.output_size = (output_size, output_size)\n",
    "\t\telse:\n",
    "\t\t\tassert len(output_size) == 2\n",
    "\t\t\tself.output_size = output_size\n",
    "\tdef __call__(self,sample):\n",
    "\t\timidx, image, label = sample['imidx'], sample['image'], sample['label']\n",
    "\n",
    "\t\tif random.random() >= 0.5:\n",
    "\t\t\timage = image[::-1]\n",
    "\t\t\tlabel = label[::-1]\n",
    "\n",
    "\t\th, w = image.shape[:2]\n",
    "\t\tnew_h, new_w = self.output_size\n",
    "\n",
    "\t\ttop = np.random.randint(0, h - new_h)\n",
    "\t\tleft = np.random.randint(0, w - new_w)\n",
    "\n",
    "\t\timage = image[top: top + new_h, left: left + new_w]\n",
    "\t\tlabel = label[top: top + new_h, left: left + new_w]\n",
    "\n",
    "\t\treturn {'imidx':imidx,'image':image, 'label':label}\n",
    "\n",
    "class ToTensor(object):\n",
    "\t\"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "\tdef __call__(self, sample):\n",
    "\n",
    "\t\timidx, image, label = sample['imidx'], sample['image'], sample['label']\n",
    "\n",
    "\t\ttmpImg = np.zeros((image.shape[0],image.shape[1],3))\n",
    "\t\ttmpLbl = np.zeros(label.shape)\n",
    "\n",
    "\t\timage = image/np.max(image)\n",
    "\t\tif(np.max(label)<1e-6):\n",
    "\t\t\tlabel = label\n",
    "\t\telse:\n",
    "\t\t\tlabel = label/np.max(label)\n",
    "\n",
    "\t\tif image.shape[2]==1:\n",
    "\t\t\ttmpImg[:,:,0] = (image[:,:,0]-0.485)/0.229\n",
    "\t\t\ttmpImg[:,:,1] = (image[:,:,0]-0.485)/0.229\n",
    "\t\t\ttmpImg[:,:,2] = (image[:,:,0]-0.485)/0.229\n",
    "\t\telse:\n",
    "\t\t\ttmpImg[:,:,0] = (image[:,:,0]-0.485)/0.229\n",
    "\t\t\ttmpImg[:,:,1] = (image[:,:,1]-0.456)/0.224\n",
    "\t\t\ttmpImg[:,:,2] = (image[:,:,2]-0.406)/0.225\n",
    "\n",
    "\t\ttmpLbl[:,:,0] = label[:,:,0]\n",
    "\n",
    "\t\t# change the r,g,b to b,r,g from [0,255] to [0,1]\n",
    "\t\t#transforms.Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225))\n",
    "\t\ttmpImg = tmpImg.transpose((2, 0, 1))\n",
    "\t\ttmpLbl = label.transpose((2, 0, 1))\n",
    "\n",
    "\t\treturn {'imidx':torch.from_numpy(imidx), 'image': torch.from_numpy(tmpImg), 'label': torch.from_numpy(tmpLbl)}\n",
    "\n",
    "class ToTensorLab(object):\n",
    "\t\"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\tdef __init__(self,flag=0):\n",
    "\t\tself.flag = flag\n",
    "\n",
    "\tdef __call__(self, sample):\n",
    "\n",
    "\t\timidx, image, label =sample['imidx'], sample['image'], sample['label']\n",
    "\n",
    "\t\ttmpLbl = np.zeros(label.shape)\n",
    "\n",
    "\t\tif(np.max(label)<1e-6):\n",
    "\t\t\tlabel = label\n",
    "\t\telse:\n",
    "\t\t\tlabel = label/np.max(label)\n",
    "\n",
    "\t\t# change the color space\n",
    "\t\tif self.flag == 2: # with rgb and Lab colors\n",
    "\t\t\ttmpImg = np.zeros((image.shape[0],image.shape[1],6))\n",
    "\t\t\ttmpImgt = np.zeros((image.shape[0],image.shape[1],3))\n",
    "\t\t\tif image.shape[2]==1:\n",
    "\t\t\t\ttmpImgt[:,:,0] = image[:,:,0]\n",
    "\t\t\t\ttmpImgt[:,:,1] = image[:,:,0]\n",
    "\t\t\t\ttmpImgt[:,:,2] = image[:,:,0]\n",
    "\t\t\telse:\n",
    "\t\t\t\ttmpImgt = image\n",
    "\t\t\ttmpImgtl = color.rgb2lab(tmpImgt)\n",
    "\n",
    "\t\t\t# nomalize image to range [0,1]\n",
    "\t\t\ttmpImg[:,:,0] = (tmpImgt[:,:,0]-np.min(tmpImgt[:,:,0]))/(np.max(tmpImgt[:,:,0])-np.min(tmpImgt[:,:,0]))\n",
    "\t\t\ttmpImg[:,:,1] = (tmpImgt[:,:,1]-np.min(tmpImgt[:,:,1]))/(np.max(tmpImgt[:,:,1])-np.min(tmpImgt[:,:,1]))\n",
    "\t\t\ttmpImg[:,:,2] = (tmpImgt[:,:,2]-np.min(tmpImgt[:,:,2]))/(np.max(tmpImgt[:,:,2])-np.min(tmpImgt[:,:,2]))\n",
    "\t\t\ttmpImg[:,:,3] = (tmpImgtl[:,:,0]-np.min(tmpImgtl[:,:,0]))/(np.max(tmpImgtl[:,:,0])-np.min(tmpImgtl[:,:,0]))\n",
    "\t\t\ttmpImg[:,:,4] = (tmpImgtl[:,:,1]-np.min(tmpImgtl[:,:,1]))/(np.max(tmpImgtl[:,:,1])-np.min(tmpImgtl[:,:,1]))\n",
    "\t\t\ttmpImg[:,:,5] = (tmpImgtl[:,:,2]-np.min(tmpImgtl[:,:,2]))/(np.max(tmpImgtl[:,:,2])-np.min(tmpImgtl[:,:,2]))\n",
    "\n",
    "\t\t\t# tmpImg = tmpImg/(np.max(tmpImg)-np.min(tmpImg))\n",
    "\n",
    "\t\t\ttmpImg[:,:,0] = (tmpImg[:,:,0]-np.mean(tmpImg[:,:,0]))/np.std(tmpImg[:,:,0])\n",
    "\t\t\ttmpImg[:,:,1] = (tmpImg[:,:,1]-np.mean(tmpImg[:,:,1]))/np.std(tmpImg[:,:,1])\n",
    "\t\t\ttmpImg[:,:,2] = (tmpImg[:,:,2]-np.mean(tmpImg[:,:,2]))/np.std(tmpImg[:,:,2])\n",
    "\t\t\ttmpImg[:,:,3] = (tmpImg[:,:,3]-np.mean(tmpImg[:,:,3]))/np.std(tmpImg[:,:,3])\n",
    "\t\t\ttmpImg[:,:,4] = (tmpImg[:,:,4]-np.mean(tmpImg[:,:,4]))/np.std(tmpImg[:,:,4])\n",
    "\t\t\ttmpImg[:,:,5] = (tmpImg[:,:,5]-np.mean(tmpImg[:,:,5]))/np.std(tmpImg[:,:,5])\n",
    "\n",
    "\t\telif self.flag == 1: #with Lab color\n",
    "\t\t\ttmpImg = np.zeros((image.shape[0],image.shape[1],3))\n",
    "\n",
    "\t\t\tif image.shape[2]==1:\n",
    "\t\t\t\ttmpImg[:,:,0] = image[:,:,0]\n",
    "\t\t\t\ttmpImg[:,:,1] = image[:,:,0]\n",
    "\t\t\t\ttmpImg[:,:,2] = image[:,:,0]\n",
    "\t\t\telse:\n",
    "\t\t\t\ttmpImg = image\n",
    "\n",
    "\t\t\ttmpImg = color.rgb2lab(tmpImg)\n",
    "\n",
    "\t\t\t# tmpImg = tmpImg/(np.max(tmpImg)-np.min(tmpImg))\n",
    "\n",
    "\t\t\ttmpImg[:,:,0] = (tmpImg[:,:,0]-np.min(tmpImg[:,:,0]))/(np.max(tmpImg[:,:,0])-np.min(tmpImg[:,:,0]))\n",
    "\t\t\ttmpImg[:,:,1] = (tmpImg[:,:,1]-np.min(tmpImg[:,:,1]))/(np.max(tmpImg[:,:,1])-np.min(tmpImg[:,:,1]))\n",
    "\t\t\ttmpImg[:,:,2] = (tmpImg[:,:,2]-np.min(tmpImg[:,:,2]))/(np.max(tmpImg[:,:,2])-np.min(tmpImg[:,:,2]))\n",
    "\n",
    "\t\t\ttmpImg[:,:,0] = (tmpImg[:,:,0]-np.mean(tmpImg[:,:,0]))/np.std(tmpImg[:,:,0])\n",
    "\t\t\ttmpImg[:,:,1] = (tmpImg[:,:,1]-np.mean(tmpImg[:,:,1]))/np.std(tmpImg[:,:,1])\n",
    "\t\t\ttmpImg[:,:,2] = (tmpImg[:,:,2]-np.mean(tmpImg[:,:,2]))/np.std(tmpImg[:,:,2])\n",
    "\n",
    "\t\telse: # with rgb color\n",
    "\t\t\ttmpImg = np.zeros((image.shape[0],image.shape[1],3))\n",
    "\t\t\timage = image/np.max(image)\n",
    "\t\t\tif image.shape[2]==1:\n",
    "\t\t\t\ttmpImg[:,:,0] = (image[:,:,0]-0.485)/0.229\n",
    "\t\t\t\ttmpImg[:,:,1] = (image[:,:,0]-0.485)/0.229\n",
    "\t\t\t\ttmpImg[:,:,2] = (image[:,:,0]-0.485)/0.229\n",
    "\t\t\telse:\n",
    "\t\t\t\ttmpImg[:,:,0] = (image[:,:,0]-0.485)/0.229\n",
    "\t\t\t\ttmpImg[:,:,1] = (image[:,:,1]-0.456)/0.224\n",
    "\t\t\t\ttmpImg[:,:,2] = (image[:,:,2]-0.406)/0.225\n",
    "\n",
    "\t\ttmpLbl[:,:,0] = label[:,:,0]\n",
    "\n",
    "\t\t# change the r,g,b to b,r,g from [0,255] to [0,1]\n",
    "\t\t#transforms.Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225))\n",
    "\t\ttmpImg = tmpImg.transpose((2, 0, 1))\n",
    "\t\ttmpLbl = label.transpose((2, 0, 1))\n",
    "\n",
    "\t\treturn {'imidx':torch.from_numpy(imidx), 'image': torch.from_numpy(tmpImg), 'label': torch.from_numpy(tmpLbl)}\n",
    "\n",
    "class SalObjDataset(Dataset):\n",
    "\tdef __init__(self,img_name_list,lbl_name_list,transform=None):\n",
    "\t\t# self.root_dir = root_dir\n",
    "\t\t# self.image_name_list = glob.glob(image_dir+'*.png')\n",
    "\t\t# self.label_name_list = glob.glob(label_dir+'*.png')\n",
    "\t\tself.image_name_list = img_name_list\n",
    "\t\tself.label_name_list = lbl_name_list\n",
    "\t\tself.transform = transform\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.image_name_list)\n",
    "\n",
    "\tdef __getitem__(self,idx):\n",
    "\n",
    "\t\t# image = Image.open(self.image_name_list[idx])#io.imread(self.image_name_list[idx])\n",
    "\t\t# label = Image.open(self.label_name_list[idx])#io.imread(self.label_name_list[idx])\n",
    "\n",
    "\t\timage = io.imread(self.image_name_list[idx])\n",
    "\t\timname = self.image_name_list[idx]\n",
    "\t\timidx = np.array([idx])\n",
    "\n",
    "\t\tif(0==len(self.label_name_list)):\n",
    "\t\t\tlabel_3 = np.zeros(image.shape)\n",
    "\t\telse:\n",
    "\t\t\tlabel_3 = io.imread(self.label_name_list[idx])\n",
    "\n",
    "\t\tlabel = np.zeros(label_3.shape[0:2])\n",
    "\t\tif(3==len(label_3.shape)):\n",
    "\t\t\tlabel = label_3[:,:,0]\n",
    "\t\telif(2==len(label_3.shape)):\n",
    "\t\t\tlabel = label_3\n",
    "\n",
    "\t\tif(3==len(image.shape) and 2==len(label.shape)):\n",
    "\t\t\tlabel = label[:,:,np.newaxis]\n",
    "\t\telif(2==len(image.shape) and 2==len(label.shape)):\n",
    "\t\t\timage = image[:,:,np.newaxis]\n",
    "\t\t\tlabel = label[:,:,np.newaxis]\n",
    "\n",
    "\t\tsample = {'imidx':imidx, 'image':image, 'label':label}\n",
    "\n",
    "\t\tif self.transform:\n",
    "\t\t\tsample = self.transform(sample)\n",
    "\n",
    "\t\treturn sample\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class REBNCONV(nn.Module):\n",
    "    def __init__(self,in_ch=3,out_ch=3,dirate=1):\n",
    "        super(REBNCONV,self).__init__()\n",
    "\n",
    "        self.conv_s1 = nn.Conv2d(in_ch,out_ch,3,padding=1*dirate,dilation=1*dirate)\n",
    "        self.bn_s1 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu_s1 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hx = x\n",
    "        xout = self.relu_s1(self.bn_s1(self.conv_s1(hx)))\n",
    "\n",
    "        return xout\n",
    "\n",
    "## upsample tensor 'src' to have the same spatial size with tensor 'tar'\n",
    "def _upsample_like(src,tar):\n",
    "\n",
    "    src = F.upsample(src,size=tar.shape[2:],mode='bilinear')\n",
    "\n",
    "    return src\n",
    "\n",
    "\n",
    "### RSU-7 ###\n",
    "class RSU7(nn.Module):#UNet07DRES(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n",
    "        super(RSU7,self).__init__()\n",
    "\n",
    "        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n",
    "        self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool3 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool4 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv5 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool5 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv6 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv7 = REBNCONV(mid_ch,mid_ch,dirate=2)\n",
    "\n",
    "        self.rebnconv6d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv5d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv4d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hx = x\n",
    "        hxin = self.rebnconvin(hx)\n",
    "\n",
    "        hx1 = self.rebnconv1(hxin)\n",
    "        hx = self.pool1(hx1)\n",
    "\n",
    "        hx2 = self.rebnconv2(hx)\n",
    "        hx = self.pool2(hx2)\n",
    "\n",
    "        hx3 = self.rebnconv3(hx)\n",
    "        hx = self.pool3(hx3)\n",
    "\n",
    "        hx4 = self.rebnconv4(hx)\n",
    "        hx = self.pool4(hx4)\n",
    "\n",
    "        hx5 = self.rebnconv5(hx)\n",
    "        hx = self.pool5(hx5)\n",
    "\n",
    "        hx6 = self.rebnconv6(hx)\n",
    "\n",
    "        hx7 = self.rebnconv7(hx6)\n",
    "\n",
    "        hx6d =  self.rebnconv6d(torch.cat((hx7,hx6),1))\n",
    "        hx6dup = _upsample_like(hx6d,hx5)\n",
    "\n",
    "        hx5d =  self.rebnconv5d(torch.cat((hx6dup,hx5),1))\n",
    "        hx5dup = _upsample_like(hx5d,hx4)\n",
    "\n",
    "        hx4d = self.rebnconv4d(torch.cat((hx5dup,hx4),1))\n",
    "        hx4dup = _upsample_like(hx4d,hx3)\n",
    "\n",
    "        hx3d = self.rebnconv3d(torch.cat((hx4dup,hx3),1))\n",
    "        hx3dup = _upsample_like(hx3d,hx2)\n",
    "\n",
    "        hx2d = self.rebnconv2d(torch.cat((hx3dup,hx2),1))\n",
    "        hx2dup = _upsample_like(hx2d,hx1)\n",
    "\n",
    "        hx1d = self.rebnconv1d(torch.cat((hx2dup,hx1),1))\n",
    "\n",
    "        return hx1d + hxin\n",
    "\n",
    "### RSU-6 ###\n",
    "class RSU6(nn.Module):#UNet06DRES(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n",
    "        super(RSU6,self).__init__()\n",
    "\n",
    "        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n",
    "        self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool3 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool4 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv5 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv6 = REBNCONV(mid_ch,mid_ch,dirate=2)\n",
    "\n",
    "        self.rebnconv5d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv4d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hx = x\n",
    "\n",
    "        hxin = self.rebnconvin(hx)\n",
    "\n",
    "        hx1 = self.rebnconv1(hxin)\n",
    "        hx = self.pool1(hx1)\n",
    "\n",
    "        hx2 = self.rebnconv2(hx)\n",
    "        hx = self.pool2(hx2)\n",
    "\n",
    "        hx3 = self.rebnconv3(hx)\n",
    "        hx = self.pool3(hx3)\n",
    "\n",
    "        hx4 = self.rebnconv4(hx)\n",
    "        hx = self.pool4(hx4)\n",
    "\n",
    "        hx5 = self.rebnconv5(hx)\n",
    "\n",
    "        hx6 = self.rebnconv6(hx5)\n",
    "\n",
    "\n",
    "        hx5d =  self.rebnconv5d(torch.cat((hx6,hx5),1))\n",
    "        hx5dup = _upsample_like(hx5d,hx4)\n",
    "\n",
    "        hx4d = self.rebnconv4d(torch.cat((hx5dup,hx4),1))\n",
    "        hx4dup = _upsample_like(hx4d,hx3)\n",
    "\n",
    "        hx3d = self.rebnconv3d(torch.cat((hx4dup,hx3),1))\n",
    "        hx3dup = _upsample_like(hx3d,hx2)\n",
    "\n",
    "        hx2d = self.rebnconv2d(torch.cat((hx3dup,hx2),1))\n",
    "        hx2dup = _upsample_like(hx2d,hx1)\n",
    "\n",
    "        hx1d = self.rebnconv1d(torch.cat((hx2dup,hx1),1))\n",
    "\n",
    "        return hx1d + hxin\n",
    "\n",
    "### RSU-5 ###\n",
    "class RSU5(nn.Module):#UNet05DRES(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n",
    "        super(RSU5,self).__init__()\n",
    "\n",
    "        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n",
    "        self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool3 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv5 = REBNCONV(mid_ch,mid_ch,dirate=2)\n",
    "\n",
    "        self.rebnconv4d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hx = x\n",
    "\n",
    "        hxin = self.rebnconvin(hx)\n",
    "\n",
    "        hx1 = self.rebnconv1(hxin)\n",
    "        hx = self.pool1(hx1)\n",
    "\n",
    "        hx2 = self.rebnconv2(hx)\n",
    "        hx = self.pool2(hx2)\n",
    "\n",
    "        hx3 = self.rebnconv3(hx)\n",
    "        hx = self.pool3(hx3)\n",
    "\n",
    "        hx4 = self.rebnconv4(hx)\n",
    "\n",
    "        hx5 = self.rebnconv5(hx4)\n",
    "\n",
    "        hx4d = self.rebnconv4d(torch.cat((hx5,hx4),1))\n",
    "        hx4dup = _upsample_like(hx4d,hx3)\n",
    "\n",
    "        hx3d = self.rebnconv3d(torch.cat((hx4dup,hx3),1))\n",
    "        hx3dup = _upsample_like(hx3d,hx2)\n",
    "\n",
    "        hx2d = self.rebnconv2d(torch.cat((hx3dup,hx2),1))\n",
    "        hx2dup = _upsample_like(hx2d,hx1)\n",
    "\n",
    "        hx1d = self.rebnconv1d(torch.cat((hx2dup,hx1),1))\n",
    "\n",
    "        return hx1d + hxin\n",
    "\n",
    "### RSU-4 ###\n",
    "class RSU4(nn.Module):#UNet04DRES(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n",
    "        super(RSU4,self).__init__()\n",
    "\n",
    "        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n",
    "        self.pool1 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "        self.pool2 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=2)\n",
    "\n",
    "        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=1)\n",
    "        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hx = x\n",
    "\n",
    "        hxin = self.rebnconvin(hx)\n",
    "\n",
    "        hx1 = self.rebnconv1(hxin)\n",
    "        hx = self.pool1(hx1)\n",
    "\n",
    "        hx2 = self.rebnconv2(hx)\n",
    "        hx = self.pool2(hx2)\n",
    "\n",
    "        hx3 = self.rebnconv3(hx)\n",
    "\n",
    "        hx4 = self.rebnconv4(hx3)\n",
    "\n",
    "        hx3d = self.rebnconv3d(torch.cat((hx4,hx3),1))\n",
    "        hx3dup = _upsample_like(hx3d,hx2)\n",
    "\n",
    "        hx2d = self.rebnconv2d(torch.cat((hx3dup,hx2),1))\n",
    "        hx2dup = _upsample_like(hx2d,hx1)\n",
    "\n",
    "        hx1d = self.rebnconv1d(torch.cat((hx2dup,hx1),1))\n",
    "\n",
    "        return hx1d + hxin\n",
    "\n",
    "### RSU-4F ###\n",
    "class RSU4F(nn.Module):#UNet04FRES(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):\n",
    "        super(RSU4F,self).__init__()\n",
    "\n",
    "        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=1)\n",
    "\n",
    "        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=1)\n",
    "        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=2)\n",
    "        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=4)\n",
    "\n",
    "        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=8)\n",
    "\n",
    "        self.rebnconv3d = REBNCONV(mid_ch*2,mid_ch,dirate=4)\n",
    "        self.rebnconv2d = REBNCONV(mid_ch*2,mid_ch,dirate=2)\n",
    "        self.rebnconv1d = REBNCONV(mid_ch*2,out_ch,dirate=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hx = x\n",
    "\n",
    "        hxin = self.rebnconvin(hx)\n",
    "\n",
    "        hx1 = self.rebnconv1(hxin)\n",
    "        hx2 = self.rebnconv2(hx1)\n",
    "        hx3 = self.rebnconv3(hx2)\n",
    "\n",
    "        hx4 = self.rebnconv4(hx3)\n",
    "\n",
    "        hx3d = self.rebnconv3d(torch.cat((hx4,hx3),1))\n",
    "        hx2d = self.rebnconv2d(torch.cat((hx3d,hx2),1))\n",
    "        hx1d = self.rebnconv1d(torch.cat((hx2d,hx1),1))\n",
    "\n",
    "        return hx1d + hxin\n",
    "\n",
    "\n",
    "##### U^2-Net ####\n",
    "class U2NET(nn.Module):\n",
    "\n",
    "    def __init__(self,in_ch=3,out_ch=1):\n",
    "        super(U2NET,self).__init__()\n",
    "\n",
    "        self.stage1 = RSU7(in_ch,32,64)\n",
    "        self.pool12 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage2 = RSU6(64,32,128)\n",
    "        self.pool23 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage3 = RSU5(128,64,256)\n",
    "        self.pool34 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage4 = RSU4(256,128,512)\n",
    "        self.pool45 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage5 = RSU4F(512,256,512)\n",
    "        self.pool56 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage6 = RSU4F(512,256,512)\n",
    "\n",
    "        # decoder\n",
    "        self.stage5d = RSU4F(1024,256,512)\n",
    "        self.stage4d = RSU4(1024,128,256)\n",
    "        self.stage3d = RSU5(512,64,128)\n",
    "        self.stage2d = RSU6(256,32,64)\n",
    "        self.stage1d = RSU7(128,16,64)\n",
    "\n",
    "        self.side1 = nn.Conv2d(64,out_ch,3,padding=1)\n",
    "        self.side2 = nn.Conv2d(64,out_ch,3,padding=1)\n",
    "        self.side3 = nn.Conv2d(128,out_ch,3,padding=1)\n",
    "        self.side4 = nn.Conv2d(256,out_ch,3,padding=1)\n",
    "        self.side5 = nn.Conv2d(512,out_ch,3,padding=1)\n",
    "        self.side6 = nn.Conv2d(512,out_ch,3,padding=1)\n",
    "\n",
    "        self.outconv = nn.Conv2d(6*out_ch,out_ch,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hx = x\n",
    "\n",
    "        #stage 1\n",
    "        hx1 = self.stage1(hx)\n",
    "        hx = self.pool12(hx1)\n",
    "\n",
    "        #stage 2\n",
    "        hx2 = self.stage2(hx)\n",
    "        hx = self.pool23(hx2)\n",
    "\n",
    "        #stage 3\n",
    "        hx3 = self.stage3(hx)\n",
    "        hx = self.pool34(hx3)\n",
    "\n",
    "        #stage 4\n",
    "        hx4 = self.stage4(hx)\n",
    "        hx = self.pool45(hx4)\n",
    "\n",
    "        #stage 5\n",
    "        hx5 = self.stage5(hx)\n",
    "        hx = self.pool56(hx5)\n",
    "\n",
    "        #stage 6\n",
    "        hx6 = self.stage6(hx)\n",
    "        hx6up = _upsample_like(hx6,hx5)\n",
    "\n",
    "        #-------------------- decoder --------------------\n",
    "        hx5d = self.stage5d(torch.cat((hx6up,hx5),1))\n",
    "        hx5dup = _upsample_like(hx5d,hx4)\n",
    "\n",
    "        hx4d = self.stage4d(torch.cat((hx5dup,hx4),1))\n",
    "        hx4dup = _upsample_like(hx4d,hx3)\n",
    "\n",
    "        hx3d = self.stage3d(torch.cat((hx4dup,hx3),1))\n",
    "        hx3dup = _upsample_like(hx3d,hx2)\n",
    "\n",
    "        hx2d = self.stage2d(torch.cat((hx3dup,hx2),1))\n",
    "        hx2dup = _upsample_like(hx2d,hx1)\n",
    "\n",
    "        hx1d = self.stage1d(torch.cat((hx2dup,hx1),1))\n",
    "\n",
    "\n",
    "        #side output\n",
    "        d1 = self.side1(hx1d)\n",
    "\n",
    "        d2 = self.side2(hx2d)\n",
    "        d2 = _upsample_like(d2,d1)\n",
    "\n",
    "        d3 = self.side3(hx3d)\n",
    "        d3 = _upsample_like(d3,d1)\n",
    "\n",
    "        d4 = self.side4(hx4d)\n",
    "        d4 = _upsample_like(d4,d1)\n",
    "\n",
    "        d5 = self.side5(hx5d)\n",
    "        d5 = _upsample_like(d5,d1)\n",
    "\n",
    "        d6 = self.side6(hx6)\n",
    "        d6 = _upsample_like(d6,d1)\n",
    "\n",
    "        d0 = self.outconv(torch.cat((d1,d2,d3,d4,d5,d6),1))\n",
    "\n",
    "        return F.sigmoid(d0), F.sigmoid(d1), F.sigmoid(d2), F.sigmoid(d3), F.sigmoid(d4), F.sigmoid(d5), F.sigmoid(d6)\n",
    "\n",
    "### U^2-Net small ###\n",
    "class U2NETP(nn.Module):\n",
    "\n",
    "    def __init__(self,in_ch=3,out_ch=1):\n",
    "        super(U2NETP,self).__init__()\n",
    "\n",
    "        self.stage1 = RSU7(in_ch,16,64)\n",
    "        self.pool12 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage2 = RSU6(64,16,64)\n",
    "        self.pool23 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage3 = RSU5(64,16,64)\n",
    "        self.pool34 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage4 = RSU4(64,16,64)\n",
    "        self.pool45 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage5 = RSU4F(64,16,64)\n",
    "        self.pool56 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n",
    "\n",
    "        self.stage6 = RSU4F(64,16,64)\n",
    "\n",
    "        # decoder\n",
    "        self.stage5d = RSU4F(128,16,64)\n",
    "        self.stage4d = RSU4(128,16,64)\n",
    "        self.stage3d = RSU5(128,16,64)\n",
    "        self.stage2d = RSU6(128,16,64)\n",
    "        self.stage1d = RSU7(128,16,64)\n",
    "\n",
    "        self.side1 = nn.Conv2d(64,out_ch,3,padding=1)\n",
    "        self.side2 = nn.Conv2d(64,out_ch,3,padding=1)\n",
    "        self.side3 = nn.Conv2d(64,out_ch,3,padding=1)\n",
    "        self.side4 = nn.Conv2d(64,out_ch,3,padding=1)\n",
    "        self.side5 = nn.Conv2d(64,out_ch,3,padding=1)\n",
    "        self.side6 = nn.Conv2d(64,out_ch,3,padding=1)\n",
    "\n",
    "        self.outconv = nn.Conv2d(6*out_ch,out_ch,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        hx = x\n",
    "\n",
    "        #stage 1\n",
    "        hx1 = self.stage1(hx)\n",
    "        hx = self.pool12(hx1)\n",
    "\n",
    "        #stage 2\n",
    "        hx2 = self.stage2(hx)\n",
    "        hx = self.pool23(hx2)\n",
    "\n",
    "        #stage 3\n",
    "        hx3 = self.stage3(hx)\n",
    "        hx = self.pool34(hx3)\n",
    "\n",
    "        #stage 4\n",
    "        hx4 = self.stage4(hx)\n",
    "        hx = self.pool45(hx4)\n",
    "\n",
    "        #stage 5\n",
    "        hx5 = self.stage5(hx)\n",
    "        hx = self.pool56(hx5)\n",
    "\n",
    "        #stage 6\n",
    "        hx6 = self.stage6(hx)\n",
    "        hx6up = _upsample_like(hx6,hx5)\n",
    "\n",
    "        #decoder\n",
    "        hx5d = self.stage5d(torch.cat((hx6up,hx5),1))\n",
    "        hx5dup = _upsample_like(hx5d,hx4)\n",
    "\n",
    "        hx4d = self.stage4d(torch.cat((hx5dup,hx4),1))\n",
    "        hx4dup = _upsample_like(hx4d,hx3)\n",
    "\n",
    "        hx3d = self.stage3d(torch.cat((hx4dup,hx3),1))\n",
    "        hx3dup = _upsample_like(hx3d,hx2)\n",
    "\n",
    "        hx2d = self.stage2d(torch.cat((hx3dup,hx2),1))\n",
    "        hx2dup = _upsample_like(hx2d,hx1)\n",
    "\n",
    "        hx1d = self.stage1d(torch.cat((hx2dup,hx1),1))\n",
    "\n",
    "\n",
    "        #side output\n",
    "        d1 = self.side1(hx1d)\n",
    "\n",
    "        d2 = self.side2(hx2d)\n",
    "        d2 = _upsample_like(d2,d1)\n",
    "\n",
    "        d3 = self.side3(hx3d)\n",
    "        d3 = _upsample_like(d3,d1)\n",
    "\n",
    "        d4 = self.side4(hx4d)\n",
    "        d4 = _upsample_like(d4,d1)\n",
    "\n",
    "        d5 = self.side5(hx5d)\n",
    "        d5 = _upsample_like(d5,d1)\n",
    "\n",
    "        d6 = self.side6(hx6)\n",
    "        d6 = _upsample_like(d6,d1)\n",
    "\n",
    "        d0 = self.outconv(torch.cat((d1,d2,d3,d4,d5,d6),1))\n",
    "\n",
    "        return F.sigmoid(d0), F.sigmoid(d1), F.sigmoid(d2), F.sigmoid(d3), F.sigmoid(d4), F.sigmoid(d5), F.sigmoid(d6)\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "\n"
   ],
   "metadata": {
    "trusted": true,
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!mkdir prediction"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!mkdir new_model"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as standard_transforms\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "# ------- 1. define loss function --------\n",
    "\n",
    "bce_loss = nn.BCELoss(size_average=True)\n",
    "\n",
    "def muti_bce_loss_fusion(d0, d1, d2, d3, d4, d5, d6, labels_v):\n",
    "\n",
    "\tloss0 = bce_loss(d0,labels_v)\n",
    "\tloss1 = bce_loss(d1,labels_v)\n",
    "\tloss2 = bce_loss(d2,labels_v)\n",
    "\tloss3 = bce_loss(d3,labels_v)\n",
    "\tloss4 = bce_loss(d4,labels_v)\n",
    "\tloss5 = bce_loss(d5,labels_v)\n",
    "\tloss6 = bce_loss(d6,labels_v)\n",
    "\n",
    "\tloss = loss0 + loss1 + loss2 + loss3 + loss4 + loss5 + loss6\n",
    "\tprint(\"l0: %3f, l1: %3f, l2: %3f, l3: %3f, l4: %3f, l5: %3f, l6: %3f\\n\"%(loss0.data.item(),loss1.data.item(),loss2.data.item(),loss3.data.item(),loss4.data.item(),loss5.data.item(),loss6.data.item()))\n",
    "\n",
    "\treturn loss0, loss\n",
    "# ------- 2. set the directory of training dataset --------\n",
    "\n",
    "model_name = 'u2net' #'u2netp'\n",
    "\n",
    "# data_dir = os.path.join(os.getcwd(), 'train_data' + os.sep)\n",
    "tra_image_dir = os.path.join('/kaggle/input/train-data-new/')\n",
    "tra_label_dir = os.path.join('/kaggle/input/train-label-new/')\n",
    "\n",
    "image_ext = '.jpg'\n",
    "label_ext = '.png'\n",
    "\n",
    "model_dir = os.path.join('/kaggle/input/saved-models/')\n",
    "\n",
    "new_model_dir = os.path.join('/kaggle/working/new_model/')\n",
    "\n",
    "epoch_num = 120\n",
    "batch_size_train = 12\n",
    "batch_size_val = 1\n",
    "train_num = 0\n",
    "val_num = 0\n",
    "\n",
    "tra_img_name_list = glob.glob(tra_image_dir + '*' + image_ext)\n",
    "\n",
    "tra_lbl_name_list = []\n",
    "for img_path in tra_img_name_list:\n",
    "\timg_name = img_path.split(os.sep)[-1]\n",
    "\n",
    "\taaa = img_name.split(\".\")\n",
    "\tbbb = aaa[0:-1]\n",
    "\timidx = bbb[0]\n",
    "\tfor i in range(1,len(bbb)):\n",
    "\t\timidx = imidx + \".\" + bbb[i]\n",
    "\n",
    "\ttra_lbl_name_list.append(tra_label_dir + imidx + label_ext)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"train images: \", len(tra_img_name_list))\n",
    "print(\"train labels: \", len(tra_lbl_name_list))\n",
    "print(\"---\")\n",
    "\n",
    "train_num = len(tra_img_name_list)\n",
    "\n",
    "salobj_dataset = SalObjDataset(\n",
    "    img_name_list=tra_img_name_list,\n",
    "    lbl_name_list=tra_lbl_name_list,\n",
    "    transform=transforms.Compose([\n",
    "        RescaleT(320),\n",
    "        RandomCrop(288),\n",
    "        ToTensorLab(flag=0)]))\n",
    "salobj_dataloader = DataLoader(salobj_dataset, batch_size=batch_size_train, shuffle=True, num_workers=1)\n",
    "\n",
    "# ------- 3. define model --------\n",
    "# define the net\n",
    "if(model_name=='u2net'):\n",
    "    net = U2NET(3, 1)\n",
    "elif(model_name=='u2netp'):\n",
    "    net = U2NETP(3,1)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "\n",
    "# ------- 4. define optimizer --------\n",
    "print(\"---define optimizer...\")\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "\n",
    "# ------- 5. training process --------\n",
    "print(\"---start training...\")\n",
    "ite_num = 0\n",
    "running_loss = 0.0\n",
    "running_tar_loss = 0.0\n",
    "ite_num4val = 0\n",
    "save_frq = 100 # save the model every 2000 iterations\n",
    "\n",
    "for epoch in range(0, epoch_num):\n",
    "    net.train()\n",
    "\n",
    "    for i, data in enumerate(salobj_dataloader):\n",
    "        ite_num = ite_num + 1\n",
    "        ite_num4val = ite_num4val + 1\n",
    "\n",
    "        inputs, labels = data['image'], data['label']\n",
    "\n",
    "        inputs = inputs.type(torch.FloatTensor)\n",
    "        labels = labels.type(torch.FloatTensor)\n",
    "\n",
    "        # wrap them in Variable\n",
    "        if torch.cuda.is_available():\n",
    "            inputs_v, labels_v = Variable(inputs.cuda(), requires_grad=False), Variable(labels.cuda(),\n",
    "                                                                                        requires_grad=False)\n",
    "        else:\n",
    "            inputs_v, labels_v = Variable(inputs, requires_grad=False), Variable(labels, requires_grad=False)\n",
    "\n",
    "        # y zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        d0, d1, d2, d3, d4, d5, d6 = net(inputs_v)\n",
    "        loss2, loss = muti_bce_loss_fusion(d0, d1, d2, d3, d4, d5, d6, labels_v)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # # print statistics\n",
    "        running_loss += loss.data.item()\n",
    "        running_tar_loss += loss2.data.item()\n",
    "\n",
    "        # del temporary outputs and loss\n",
    "        del d0, d1, d2, d3, d4, d5, d6, loss2, loss\n",
    "\n",
    "        print(\"[epoch: %3d/%3d, batch: %5d/%5d, ite: %d] train loss: %3f, tar: %3f \" % (\n",
    "        epoch + 1, epoch_num, (i + 1) * batch_size_train, train_num, ite_num, running_loss / ite_num4val, running_tar_loss / ite_num4val))\n",
    "\n",
    "        if ite_num % save_frq == 0:\n",
    "\n",
    "            torch.save(net.state_dict(), new_model_dir + model_name+'.pth')\n",
    "            running_loss = 0.0\n",
    "            running_tar_loss = 0.0\n",
    "            net.train()  # resume train\n",
    "            ite_num4val = 0"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/working'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from skimage import io, transform\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms#, utils\n",
    "# import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "# from data_loader import RescaleT\n",
    "# from data_loader import ToTensor\n",
    "# from data_loader import ToTensorLab\n",
    "# from data_loader import SalObjDataset\n",
    "\n",
    "# from model import U2NET # full size version 173.6 MB\n",
    "# from model import U2NETP # small version u2net 4.7 MB\n",
    "\n",
    "# normalize the predicted SOD probability map\n",
    "def normPRED(d):\n",
    "    ma = torch.max(d)\n",
    "    mi = torch.min(d)\n",
    "\n",
    "    dn = (d-mi)/(ma-mi)\n",
    "\n",
    "    return dn\n",
    "\n",
    "def save_output(image_name,pred,d_dir):\n",
    "\n",
    "    predict = pred\n",
    "    predict = predict.squeeze()\n",
    "    predict_np = predict.cpu().data.numpy()\n",
    "\n",
    "    im = Image.fromarray(predict_np*255).convert('RGB')\n",
    "    img_name = image_name.split(os.sep)[-1]\n",
    "    image = io.imread(image_name)\n",
    "    imo = im.resize((image.shape[1],image.shape[0]),resample=Image.BILINEAR)\n",
    "\n",
    "    pb_np = np.array(imo)\n",
    "\n",
    "    aaa = img_name.split(\".\")\n",
    "    bbb = aaa[0:-1]\n",
    "    imidx = bbb[0]\n",
    "    \n",
    "    for i in range(1,len(bbb)):\n",
    "        imidx = imidx + \".\" + bbb[i]\n",
    "\n",
    "    imo.save(d_dir+imidx+'.png')\n",
    "\n",
    "def main():\n",
    "\n",
    "    # --------- 1. get image path and name ---------\n",
    "    model_name='u2net'#u2netp\n",
    "\n",
    "\n",
    "\n",
    "    image_dir = os.path.join('../input/fuzzystack-covid/')\n",
    "    prediction_dir = os.path.join('/kaggle/working/prediction/')\n",
    "    test_model_dir = os.path.join(new_model_dir, model_name + '.pth')\n",
    "\n",
    "    img_name_list = glob.glob(image_dir + os.sep + '*')\n",
    "    print(img_name_list)\n",
    "\n",
    "    # --------- 2. dataloader ---------\n",
    "    #1. dataloader\n",
    "    test_salobj_dataset = SalObjDataset(img_name_list = img_name_list,\n",
    "                                        lbl_name_list = [],\n",
    "                                        transform=transforms.Compose([RescaleT(320),\n",
    "                                                                      ToTensorLab(flag=0)])\n",
    "                                        )\n",
    "    test_salobj_dataloader = DataLoader(test_salobj_dataset,\n",
    "                                        batch_size=1,\n",
    "                                        shuffle=False,\n",
    "                                        num_workers=1)\n",
    "\n",
    "    # --------- 3. model define ---------\n",
    "    if(model_name=='u2net'):\n",
    "        print(\"...load U2NET---173.6 MB\")\n",
    "        net = U2NET(3,1)\n",
    "    elif(model_name=='u2netp'):\n",
    "        print(\"...load U2NEP---4.7 MB\")\n",
    "        net = U2NETP(3,1)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        net.load_state_dict(torch.load(test_model_dir))\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.load_state_dict(torch.load(test_model_dir, map_location='cpu'))\n",
    "    net.eval()\n",
    "\n",
    "    # --------- 4. inference for each image ---------\n",
    "    for i_test, data_test in enumerate(test_salobj_dataloader):\n",
    "\n",
    "        print(\"inferencing:\",img_name_list[i_test].split(os.sep)[-1])\n",
    "\n",
    "        inputs_test = data_test['image']\n",
    "        inputs_test = inputs_test.type(torch.FloatTensor)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            inputs_test = Variable(inputs_test.cuda())\n",
    "        else:\n",
    "            inputs_test = Variable(inputs_test)\n",
    "\n",
    "        d1,d2,d3,d4,d5,d6,d7= net(inputs_test)\n",
    "\n",
    "        # normalization\n",
    "        pred = d1[:,0,:,:]\n",
    "        pred = normPRED(pred)\n",
    "\n",
    "        # save results to test_results folder\n",
    "        if not os.path.exists(prediction_dir):\n",
    "            os.makedirs(prediction_dir, exist_ok=True)\n",
    "        save_output(img_name_list[i_test],pred,prediction_dir)\n",
    "\n",
    "        del d1,d2,d3,d4,d5,d6,d7\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def normPRED(d):\n",
    "    ma = torch.max(d)\n",
    "    mi = torch.min(d)\n",
    "\n",
    "    dn = (d-mi)/(ma-mi)\n",
    "\n",
    "    return dn\n",
    "\n",
    "def save_output(image_name,pred,d_dir):\n",
    "\n",
    "    predict = pred\n",
    "    predict = predict.squeeze()\n",
    "    predict_np = predict.cpu().data.numpy()\n",
    "\n",
    "    im = Image.fromarray(predict_np*255).convert('RGB')\n",
    "    img_name = image_name.split(os.sep)[-1]\n",
    "    image = io.imread(image_name)\n",
    "    imo = im.resize((image.shape[1],image.shape[0]),resample=Image.BILINEAR)\n",
    "\n",
    "    pb_np = np.array(imo)\n",
    "\n",
    "    aaa = img_name.split(\".\")\n",
    "    bbb = aaa[0:-1]\n",
    "    imidx = bbb[0]\n",
    "    for i in range(1,len(bbb)):\n",
    "        imidx = imidx + \".\" + bbb[i]\n",
    "\n",
    "    imo.save(d_dir+imidx+'.png')\n",
    "\n",
    "def main():\n",
    "\n",
    "    # --------- 1. get image path and name ---------\n",
    "    model_name='u2net'#u2netp\n",
    "\n",
    "\n",
    "\n",
    "    image_dir = os.path.join('/kaggle/input/test-data-normal/')\n",
    "    prediction_dir = os.path.join('/kaggle/working/prediction/')\n",
    "    test_model_dir = os.path.join(new_model_dir, model_name + '.pth')\n",
    "\n",
    "    img_name_list = glob.glob(image_dir + os.sep + '*')\n",
    "    print(img_name_list)\n",
    "\n",
    "    # --------- 2. dataloader ---------\n",
    "    #1. dataloader\n",
    "    test_salobj_dataset = SalObjDataset(img_name_list = img_name_list,\n",
    "                                        lbl_name_list = [],\n",
    "                                        transform=transforms.Compose([RescaleT(320),\n",
    "                                                                      ToTensorLab(flag=0)])\n",
    "                                        )\n",
    "    test_salobj_dataloader = DataLoader(test_salobj_dataset,\n",
    "                                        batch_size=1,\n",
    "                                        shuffle=False,\n",
    "                                        num_workers=1)\n",
    "\n",
    "    # --------- 3. model define ---------\n",
    "    if(model_name=='u2net'):\n",
    "        print(\"...load U2NET---173.6 MB\")\n",
    "        net = U2NET(3,1)\n",
    "    elif(model_name=='u2netp'):\n",
    "        print(\"...load U2NEP---4.7 MB\")\n",
    "        net = U2NETP(3,1)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        net.load_state_dict(torch.load(test_model_dir))\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.load_state_dict(torch.load(test_model_dir, map_location='cpu'))\n",
    "    net.eval()\n",
    "\n",
    "    # --------- 4. inference for each image ---------\n",
    "    for i_test, data_test in enumerate(test_salobj_dataloader):\n",
    "\n",
    "        print(\"inferencing:\",img_name_list[i_test].split(os.sep)[-1])\n",
    "\n",
    "        inputs_test = data_test['image']\n",
    "        inputs_test = inputs_test.type(torch.FloatTensor)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            inputs_test = Variable(inputs_test.cuda())\n",
    "        else:\n",
    "            inputs_test = Variable(inputs_test)\n",
    "\n",
    "        d1,d2,d3,d4,d5,d6,d7= net(inputs_test)\n",
    "\n",
    "        # normalization\n",
    "        pred = d1[:,0,:,:]\n",
    "        pred = normPRED(pred)\n",
    "\n",
    "        # save results to test_results folder\n",
    "        if not os.path.exists(prediction_dir):\n",
    "            os.makedirs(prediction_dir, exist_ok=True)\n",
    "        save_output(img_name_list[i_test],pred,prediction_dir)\n",
    "\n",
    "        del d1,d2,d3,d4,d5,d6,d7\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt # plt 用于显示图片\n",
    "import matplotlib.image as mpimg # mpimg 用于读取图片\n",
    "\n",
    "\n",
    "lena = mpimg.imread('/kaggle/working/prediction/COVID_61.png') # 读取和代码处于同一目录下的 lena.png\n",
    "# 此时 lena 就已经是一个 np.array 了，可以对它进行任意处理\n",
    "lena.shape #(512, 512, 3)\n",
    "\n",
    "plt.imshow(lena) # 显示图片\n",
    "plt.axis('off') # 不显示坐标轴\n",
    "plt.show()\n",
    "olena = mpimg.imread('/kaggle/input/test-data/azqsjgmero.jpg') # 读取和代码处于同一目录下的 lena.png\n",
    "# 此时 lena 就已经是一个 np.array 了，可以对它进行任意处理\n",
    "olena.shape #(512, 512, 3)\n",
    "\n",
    "plt.imshow(olena) # 显示图片\n",
    "plt.axis('off') # 不显示坐标轴\n",
    "plt.show()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lena = mpimg.imread('/kaggle/working/prediction/azqsjgmero.png') # 读取和代码处于同一目录下的 lena.png\n",
    "# 此时 lena 就已经是一个 np.array 了，可以对它进行任意处理\n",
    "lena.shape #(512, 512, 3)\n",
    "\n",
    "plt.imshow(lena) # 显示图片\n",
    "plt.axis('off') # 不显示坐标轴\n",
    "plt.show()\n",
    "olena = mpimg.imread('/kaggle/input/test-data/azqsjgmero.jpg') # 读取和代码处于同一目录下的 lena.png\n",
    "# 此时 lena 就已经是一个 np.array 了，可以对它进行任意处理\n",
    "olena.shape #(512, 512, 3)\n",
    "\n",
    "plt.imshow(olena) # 显示图片\n",
    "plt.axis('off') # 不显示坐标轴\n",
    "plt.show()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lena = mpimg.imread('/kaggle/working/prediction/ayxpvtjqhh.png') # 读取和代码处于同一目录下的 lena.png\n",
    "# 此时 lena 就已经是一个 np.array 了，可以对它进行任意处理\n",
    "lena.shape #(512, 512, 3)\n",
    "\n",
    "plt.imshow(lena) # 显示图片\n",
    "plt.axis('off') # 不显示坐标轴\n",
    "plt.show()\n",
    "olena = mpimg.imread('/kaggle/input/test-data/ayxpvtjqhh.jpg') # 读取和代码处于同一目录下的 lena.png\n",
    "# 此时 lena 就已经是一个 np.array 了，可以对它进行任意处理\n",
    "olena.shape #(512, 512, 3)\n",
    "plt.imshow(olena) # 显示图片\n",
    "plt.axis('off') # 不显示坐标轴\n",
    "plt.show()\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lena = mpimg.imread('/kaggle/working/prediction/azfpmjtftn.png') # 读取和代码处于同一目录下的 lena.png\n",
    "# 此时 lena 就已经是一个 np.array 了，可以对它进行任意处理\n",
    "lena.shape #(512, 512, 3)\n",
    "\n",
    "plt.imshow(lena) # 显示图片\n",
    "plt.axis('off') # 不显示坐标轴\n",
    "plt.show()\n",
    "olena = mpimg.imread('/kaggle/input/test-data/azfpmjtftn.jpg') # 读取和代码处于同一目录下的 lena.png\n",
    "# 此时 lena 就已经是一个 np.array 了，可以对它进行任意处理\n",
    "olena.shape #(512, 512, 3)\n",
    "plt.imshow(olena) # 显示图片\n",
    "plt.axis('off') # 不显示坐标轴\n",
    "plt.show()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lena = mpimg.imread('/kaggle/working/prediction/fcltgaaded.png') # 读取和代码处于同一目录下的 lena.png\n",
    "# 此时 lena 就已经是一个 np.array 了，可以对它进行任意处理\n",
    "lena.shape #(512, 512, 3)\n",
    "\n",
    "plt.imshow(lena) # 显示图片\n",
    "plt.axis('off') # 不显示坐标轴\n",
    "plt.show()\n",
    "olena = mpimg.imread('/kaggle/input/test-data/fcltgaaded.jpg') # 读取和代码处于同一目录下的 lena.png\n",
    "# 此时 lena 就已经是一个 np.array 了，可以对它进行任意处理\n",
    "olena.shape #(512, 512, 3)\n",
    "plt.imshow(olena) # 显示图片\n",
    "plt.axis('off') # 不显示坐标轴\n",
    "plt.show()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lena = mpimg.imread('/kaggle/working/prediction/fdsusaiebh.png') # 读取和代码处于同一目录下的 lena.png\n",
    "# 此时 lena 就已经是一个 np.array 了，可以对它进行任意处理\n",
    "lena.shape #(512, 512, 3)\n",
    "\n",
    "plt.imshow(lena) # 显示图片\n",
    "plt.axis('off') # 不显示坐标轴\n",
    "plt.show()\n",
    "olena = mpimg.imread('/kaggle/input/test-data/fdsusaiebh.jpg') # 读取和代码处于同一目录下的 lena.png\n",
    "# 此时 lena 就已经是一个 np.array 了，可以对它进行任意处理\n",
    "olena.shape #(512, 512, 3)\n",
    "plt.imshow(olena) # 显示图片\n",
    "plt.axis('off') # 不显示坐标轴\n",
    "plt.show()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lena = mpimg.imread('/kaggle/working/prediction/fflshfyyvz.png') # 读取和代码处于同一目录下的 lena.png\n",
    "# 此时 lena 就已经是一个 np.array 了，可以对它进行任意处理\n",
    "lena.shape #(512, 512, 3)\n",
    "\n",
    "plt.imshow(lena) # 显示图片\n",
    "plt.axis('off') # 不显示坐标轴\n",
    "plt.show()\n",
    "olena = mpimg.imread('/kaggle/input/test-data/fflshfyyvz.jpg') # 读取和代码处于同一目录下的 lena.png\n",
    "# 此时 lena 就已经是一个 np.array 了，可以对它进行任意处理\n",
    "olena.shape #(512, 512, 3)\n",
    "plt.imshow(olena) # 显示图片\n",
    "plt.axis('off') # 不显示坐标轴\n",
    "plt.show()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
